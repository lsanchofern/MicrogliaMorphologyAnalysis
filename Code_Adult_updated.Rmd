---
title: "R Notebook"
output: html_notebook
---
This code is for analysis of microglia morphology in 4 experimental groups (tdt no MD, Tdt MD, Cyr61 no MD, Cyr61 MD). It uses Python for PCA and K-means clustering and then R for GLM model comparison. 
```{python}
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
```
```{python}
#import WT files from working directory 
WT_nuc = pd.read_csv('./data/WT_MyExpt_all_updatedIba1_nuc.csv')
WT_nuc=WT_nuc[WT_nuc.Children_Microglia_Count>0] #filter so you only have the ones with actual processes traced 
WT_nuc=WT_nuc.add_prefix('soma_')

WT_microglia = pd.read_csv('./data/WT_MyExpt_all_updatedMicroglia.csv')
WT_microglia.dropna()
WT_data= pd.concat([WT_nuc.reset_index(drop=True),WT_microglia.reset_index(drop=True)], axis=1)

#import WT MD files 
WTMD_nuc = pd.read_csv('./data/WTMD_MyExpt_all_updatedIba1_nuc.csv')
WTMD_nuc=WTMD_nuc[WTMD_nuc.Children_Microglia_Count>0]
WTMD_nuc.dropna()
WTMD_microglia = pd.read_csv('./data/WTMD_MyExpt_all_updatedMicroglia.csv')
WTMD_microglia.dropna()
WTMD_nuc=WTMD_nuc.add_prefix('soma_')
WTMD_data= pd.concat([WTMD_nuc.reset_index(drop=True),WTMD_microglia.reset_index(drop=True)], axis=1)

#import KO files 
KO_nuc = pd.read_csv('./data/KO_MyExpt_all_updated_Iba1_nuc.csv')
KO_nuc=KO_nuc[KO_nuc.Children_Microglia_Count>0] 
KO_microglia = pd.read_csv('./data/KO_MyExpt_all_updated_Microglia.csv')
KO_nuc=KO_nuc.add_prefix('soma_')

KO_data= pd.concat([KO_nuc.reset_index(drop=True),KO_microglia.reset_index(drop=True)], axis=1)

#import KO MD files 
KOMD_nuc = pd.read_csv('./data/KOMD_MyExpt_all_updated_Iba1_nuc.csv')
KOMD_nuc=KOMD_nuc[KOMD_nuc.Children_Microglia_Count>0]
KOMD_microglia = pd.read_csv('./data/KOMD_MyExpt_all_updated_Microglia.csv')
KOMD_nuc=KOMD_nuc.add_prefix('soma_')

KOMD_data= pd.concat([KOMD_nuc.reset_index(drop=True),KOMD_microglia.reset_index(drop=True)], axis=1)
```
```{python}
#create mask for removing zeros (we don't want microglia with zeros branches)
WT_data=WT_data[WT_data.soma_ObjectSkeleton_NumberBranchEnds_MorphologicalSkeleton>0]
WTMD_data=WTMD_data[WTMD_data.soma_ObjectSkeleton_NumberBranchEnds_MorphologicalSkeleton>0]
KO_data=KO_data[KO_data.soma_ObjectSkeleton_NumberBranchEnds_MorphologicalSkeleton>0]
KOMD_data=KOMD_data[KOMD_data.soma_ObjectSkeleton_NumberBranchEnds_MorphologicalSkeleton>0]


```

```{python}
#concatenation all dataframes into a big one 

data1=pd.concat([WT_data,WTMD_data],ignore_index=True,axis=0)
data2=pd.concat([data1,KO_data],ignore_index=True,axis=0)
data=pd.concat([data2,KOMD_data],ignore_index=True,axis=0)

```
```{python}
#convert  from pixels to microns 
data['soma_AreaShape_Area']*= .0441
data['AreaShape_Area']*= .0441
data['AreaShape_BoundingBoxArea']*= .0441
data['AreaShape_ConvexArea']*=.0441

data['soma_ObjectSkeleton_TotalObjectSkeletonLength_MorphologicalSkeleton']*= .21
data['soma_averagebranchlength']= data['soma_ObjectSkeleton_TotalObjectSkeletonLength_MorphologicalSkeleton']/ (data['soma_ObjectSkeleton_NumberNonTrunkBranches_MorphologicalSkeleton']+ data['soma_ObjectSkeleton_NumberTrunks_MorphologicalSkeleton'])

data['soma_AreaShape_MajorAxisLength']*=0.21
data['soma_AreaShape_MinorAxisLength']*=0.21

data['AreaShape_MajorAxisLength']*=0.21
data['AreaShape_MinorAxisLength']*=0.21
data['AreaShape_MaximumRadius']*=0.21
data['AreaShape_MeanRadius']*=0.21
data['AreaShape_Perimeter']*=0.21

```
```{python}
data.soma_AreaShape_MajorAxisLength
data.soma_averagebranchlength
data.soma_AreaShape_Eccentricity
data.columns.values.tolist()
data.tail()

```


```{python}
#basic descriptive statistics for data exploration 
grouped_mean=data.groupby(['soma_Metadata_Mouse','soma_Group'],as_index=False).mean()
grouped_std=data.groupby(['soma_Metadata_Mouse','soma_Group'],as_index=False).std()
grouped_median=data.groupby(['soma_Metadata_Mouse','soma_Group'],as_index=False).median()

grouped2_mean=grouped_mean.groupby(['soma_Group'],as_index=False).mean()
grouped2_std=grouped_std.groupby(['soma_Group'],as_index=False).std()
grouped2_median=grouped_median.groupby(['soma_Group'],as_index=False).median()

```
```{python}
# use seaborn to make a violin chart 
import seaborn as sns 
plt.figure(figsize=(12,10))
sns.set_theme(style="whitegrid")
sns.set_context("talk")

#custom color palette 
my_palette=['dimgray', 'darkviolet', 'lightgray', 'violet']
sns.violinplot(x="soma_Group",
            y="soma_AreaShape_Eccentricity",
            data=data,
            palette=my_palette,
            orient='v', 
            order=['WT', 'KO','WT_MD','KO_MD'],
            inner="box")

plt.title("Soma Eccentricity")
plt.ylim(0,1)
plt.show()      
```

```{python}
# now we import modules for classifying and PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

#encode viral group 
label_encoder=LabelEncoder()
data['encoded_group']=label_encoder.fit_transform(data['soma_Group'].astype(str))

#define features (aka variables) of interest for our model 
features=['soma_AreaShape_Area',
          'soma_AreaShape_Eccentricity', 
          'soma_AreaShape_MinorAxisLength',
          'soma_AreaShape_MajorAxisLength',
          'soma_ObjectSkeleton_TotalObjectSkeletonLength_MorphologicalSkeleton',
          'soma_ObjectSkeleton_NumberBranchEnds_MorphologicalSkeleton',
          'soma_ObjectSkeleton_NumberNonTrunkBranches_MorphologicalSkeleton',
          'soma_ObjectSkeleton_NumberTrunks_MorphologicalSkeleton',
          'soma_averagebranchlength',
          'AreaShape_Area',
          'AreaShape_BoundingBoxArea',
          'AreaShape_ConvexArea',
          'AreaShape_FormFactor',
          'AreaShape_Compactness',
          'AreaShape_Eccentricity',
          'AreaShape_MajorAxisLength',
          'AreaShape_MinorAxisLength',
          'AreaShape_MaximumRadius',
          'AreaShape_MeanRadius',
          'AreaShape_Perimeter',
          'AreaShape_Solidity']

         
```
```{python}
#split data into experimental groups 

#data=data.reset_index(drop=True) #reindex dataframe
#standard scaler
data=data.reset_index(drop=True) #reindex dataframe

scaler=StandardScaler()
cleaned=data[features]
cleaned.dropna()
segmentation_std=scaler.fit_transform(cleaned)
print(segmentation_std)

segmentation_std
np.isnan(segmentation_std.any())
```
```{python}
#Dimensionality Reduction with PCA 
pca=PCA()
pca.fit(segmentation_std)

pca.explained_variance_ratio_ #how much of the variance is explained by each of the  individual components 
colNames=list(features)
#print(colNames)

```
```{python}
#AllPCs=pd.DataFrame(pca.components_,columns=colNames, 
         #          index=['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8','PC-9'])


```
```{python}
# PLOTTING TO DETERMINE NUMBER OF COMPONENTS TO USE 
plt.figure(figsize=(12,8))
plt.plot(range(1,22), pca.explained_variance_ratio_.cumsum(), marker = 'o', linestyle='--')
plt.title('Explained Variance by Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()
```
Ok so from this we want 4 components because that explains 80% of the varaiblility

```{python}
#now we perform PCA with the chosen number of components (4)
pca=PCA(n_components=4)

#fit the model to our data with the selected number of components (4)
pca.fit(segmentation_std)
scores_pca=pca.transform(segmentation_std)
scores_pca
#df_PCA=pd.DataFrame(pca.components_, columns=['PC1','PC2','PC3','PC4'])

```
```{python}
#Feature importance for each PC
pca.explained_variance_ratio_
print(abs(pca.components_))

df_pca=pd.DataFrame(abs(pca.components_.T), columns= ['PC1','PC2','PC3','PC4'],
    index= features)


#make clustermap heatmap 
plt.figure(figsize=(10,15))
sns.heatmap(df_pca, annot=True)
plt.rcParams['pdf.use14corefonts'] = True

plt.savefig('PCA_features',format='pdf', bbox_inches="tight")
plt.show()
```

```{python}
#how many k clusters will we use? 

wcss=[]; #within cluster sum of squares 
for i in range(1,21):
    kmeans_pca=KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans_pca.fit(scores_pca)
    wcss.append(kmeans_pca.inertia_)
```
```{python}
#plot to use the elbow method for cluster number selection 
plt.figure(figsize=(10,8))
plt.plot(range(1,21), wcss, marker='o', linestyle= '--')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('K-means with PCA clustering')
plt.show()
```
Gonna choose 4 clusters, which makes sense given the literature 
```{python}
#KMEANS CLUSTERING WITH CHOSEN NUMBER OF CLUSTERS AND USING PCA COMPONENTS
kmeans_pca=KMeans(n_clusters=4, init='k-means++', random_state=42)

#we fit data with the k-means pca model
kmeans_pca.fit(scores_pca)
```
```{python}
#create new data frame with the original features and add the PCA scores and assigned clusters. 
df_cleaned=data[features]
df_segm_pca_kmeans=pd.concat([df_cleaned.reset_index(drop=True), pd.DataFrame(scores_pca)], axis=1)
df_segm_pca_kmeans.columns.values[-4:]=['Component 1','Component 2', 'Component 3','Component 4']
#add viral group
df_segm_pca_kmeans['Group']=data['soma_Group']
#add last column with pca k-means clustering labels 
df_segm_pca_kmeans['Segment K-means PCA']=kmeans_pca.labels_
df_segm_pca_kmeans.head()

```
```{python}

#"segment"=cluster
df_segm_pca_kmeans['Segment']=df_segm_pca_kmeans['Segment K-means PCA'].map({0:'first',
                                                                            1:'second',
                                                                            2:'third',
                                                                            3:'fourth'
                                                                            })
df_segm_pca_kmeans.head()

```
```{python}
# plot the data by the PCA components 

x_axis=df_segm_pca_kmeans['Component 2']
y_axis=df_segm_pca_kmeans['Component 1']

plt.figure(figsize=(10,8))
sns.scatterplot(data=df_segm_pca_kmeans, x=x_axis, y=y_axis,
                hue_order=['first','second','third','fourth'], hue = df_segm_pca_kmeans['Segment'])
plt.title('Clusters by PCA components')
plt.show()
```
```{python}
#mean components values of each cluster
#mean components values of each cluster

cluster1_means=df_segm_pca_kmeans.loc[df_segm_pca_kmeans['Segment']== 'first'].mean()
cluster2_means=df_segm_pca_kmeans.loc[df_segm_pca_kmeans['Segment']== 'second'].mean()
cluster3_means=df_segm_pca_kmeans.loc[df_segm_pca_kmeans['Segment']== 'third'].mean()
cluster4_means=df_segm_pca_kmeans.loc[df_segm_pca_kmeans['Segment']== 'fourth'].mean()

#make dataframes with all averages based on cluster 
DF1=pd.concat([cluster1_means,cluster2_means],axis=1)
DF2=pd.concat([DF1,cluster3_means],axis=1)
DF3=pd.concat([DF2,cluster4_means],axis=1)
cluster_averages=DF3
cluster_averages.columns=['Cluster 1','Cluster 2','Cluster 3', 'Cluster 4']


```
```{python}
#some data frame clean up 
#delete encoded_virus, Component1, PC2 PC3, and Segment K-mean PCA 
#you don't need these for clustermap 

cleaned_cluster_averages=cluster_averages.drop(index=['Component 1', 'Component 2','Component 3', 'Component 4', 'Segment K-means PCA'])
print(cleaned_cluster_averages)
df_segm_pca_kmeans['Mouse']=data['soma_Metadata_Mouse']
print(df_segm_pca_kmeans)

```
```{python}
#plotting the cluster map for each group 
#I am not using the scaled features, but using the raw values because some of the morphotypes have already been quantified in the literature, so I want to see the actual numbers 

from matplotlib.colors import LogNorm
sns.set(font_scale=1.0)

#tdt no MD
plt.figure(figsize=(10,8))
sns.clustermap(cleaned_cluster_averages, norm=LogNorm())
plt.title('Tdt no MD')
plt.show()
plt.savefig('cleaned_clustermap',format='pdf', bbox_inches="tight")

```
```{python}
#use the scaled measurements 
colNames=list(features)

df_scaled=pd.DataFrame(segmentation_std,columns=colNames)

#add the assigned clusters
#tdt no MD
#add last column with pca k-means clustering labels 
df_scaled['Cluster']=kmeans_pca.labels_


#averages of features per cluster 
cluster_scaled_means=df_scaled.groupby(['Cluster']).mean()
cluster_scaled_means
#make 
```


```{python}
#plot the clusters using scaled measurements 
#tdt no MD
plt.figure(figsize=(10,15))
sns.clustermap(cluster_scaled_means.T)#transpose so that cluster is on x axis 
plt.rcParams['pdf.use14corefonts'] = True

plt.savefig('scaled_clustermap',format='pdf', bbox_inches="tight")
plt.show()

```
```{python}
#make facetgrid violin plot of all selected features per Group. 

sns.set_theme("talk")
sns.set_theme(style="ticks")

plt.figure().clf()

for j in range(21): 
  sns.violinplot(data=data,x='soma_Group',y=features[j],
            order=['WT', 'KO','WT_MD','KO_MD'],
            palette=my_palette,inner="quart")
  plt.title(features[j])          
  sns.despine() 
  plt.show()
  plt.savefig(str(features[j])+ '_violinplot',format='pdf', bbox_inches="tight")
  plt.figure().clf()


  
```


```{python}
import scipy.stats as stats
import scikit_posthocs as sp 

pval_kruskal=[] 
pval_featurename=[]
pval_dunn=[]

for j in range (21):   
  #kruskal wallis test 
  result, f=stats.kruskal(data[features[j]].loc[data['soma_Group']=='WT'], data[features[j]].loc[data['soma_Group']=='WT_MD'],
      data[features[j]].loc[data['soma_Group']=='KO'],
      data[features[j]].loc[data['soma_Group']=='KO_MD'])
  pval_featurename.append(features[j])
  pval_kruskal.append(f)    
  #dunn's posthoc tests 
  temp=[data[features[j]].loc[data['soma_Group']=='WT'], data[features[j]].loc[data['soma_Group']=='WT_MD'],
      data[features[j]].loc[data['soma_Group']=='KO'],
      data[features[j]].loc[data['soma_Group']=='KO_MD']]
  
  p= sp.posthoc_dunn(temp, p_adjust='holm')
  p.index.names=[features[j]]
  p.rename(index={1:'WT',2:'WT_MD',3:'KO',4:'KO_MD'},inplace=True)
  pval_dunn.append(p)

#make dataframe with Kruskal wallis results   
pvals_kruskal_adult=pd.DataFrame(pval_kruskal, index=pval_featurename)

df_pval_dunn=pd.concat(pval_dunn, 
  keys=map(lambda d: d.index.name, pval_dunn), axis=0)
df_pval_dunn.columns=['WT','WT_MD','KO','KO_MD']

pvals_kruskal_adult.to_csv('./PVals_Kruskal_adult.csv')
df_pval_dunn.to_csv('./Dunns_Kruskal_adult.csv')


```

```{python}
stats.kruskal(data[features[20]].loc[data['soma_Group']=='WT'], data[features[20]].loc[data['soma_Group']=='WT_MD'],
      data[features[20]].loc[data['soma_Group']=='KO'],
      data[features[20]].loc[data['soma_Group']=='KO_MD'])

```
```{python}
#number of microglia per group 
def groupcount(data,group):
    #data = data frame 
    #segment= "Group" , epxeirmental condition (i.e WT or KO_MD)
    #get cluster percent per mouse 
    microglia_count=(data.groupby([])[group].count())
    return microglia_count 
```

```{python}
microglia_count=data.groupby(['soma_Group'])['soma_Group'].count()
print(microglia_count)
```

```{python}
def clusterpercentage (data, segment):
    #data = data frame 
    #segment= "Segment" column with cluster IDs 
    #get cluster percent per mouse 
    percent=(data.groupby(['Group','Mouse',segment])[segment].count())/(data.groupby(['Mouse'])['Mouse'].count())
    return percent 

```
```{python}
percent=clusterpercentage(df_segm_pca_kmeans,'Segment')
percent
```
```{python}

df_percent=pd.DataFrame(percent, columns=['percent'])
df_percent=df_percent.reset_index(level=['Group','Mouse','Segment'])
df_percent['Segment']=df_percent['Segment'].replace({"first":"ramified","second":"rod-like","third": "hyper-ramified","fourth":"amoeboid"})
print(df_percent)
df_percent.to_csv('./adult_all_data.csv')

```

```{r}
library(tidyverse)
library(glmmTMB)
```
```{r}
all_data=read.csv("adult_all_data.csv")
print(all_data)

```
```{r}
#linear mixed effects model 
library(glmmTMB)
all_data$Mouse <-factor(all_data$Mouse)
all_data$Segment<-factor(all_data$Segment)
all_data$Group<-factor(all_data$Group)


model="percent ~ Segment*Group +(1|Mouse)"
posthoc1= "~Segment|Group"
posthoc2="~Group|Segment"
y.model=as.character(model)
z.model=as.character(posthoc1)
x.model=as.character(posthoc2)

options(contrasts=c("contr.sum","contr.poly"))
model <-glmmTMB(as.formula(paste(y.model)), data=all_data, family=beta_family(link='logit'))


```
```{r}
#look at the model 
library(DHARMa)
res=simulateResiduals(model)
plot(res,rank=T)
modelcheck<-recordPlot()
plot.new()
modelcheck
print(model)
```
```{r}
#anova 

anova=car::Anova(model)
anova
```
```{r}
# posthoc1 test looking at group 
library(emmeans)
sink(file="pairwise_tests_adult.txt")

ph<-emmeans(model, as.formula(paste(z.model)))
ph2<-contrast(ph,method="pairwise", adjust="none")
ph3<-test(ph2,by=NULL,adjust="sidak")
posthoc1<-as.data.frame(ph3)
posthoc1

ph1<-emmeans(model, as.formula(paste(x.model)))
ph4<-contrast(ph1,method="pairwise", adjust="none")
ph5<-test(ph4,by=NULL,adjust="sidak")
posthoc2<-as.data.frame(ph5)
posthoc2
sink(file=NULL)
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd 
import numpy as np

all_data = pd.read_csv('./adult_all_data.csv')

plt.figure(figsize=(12,10))

sns.set_theme("talk")
sns.set_theme(style="ticks")
hue_order=['WT','KO','WT_MD','KO_MD']
ax=sns.boxplot(x='Segment',y='percent',hue='Group',data=all_data,palette=my_palette,
            whis=[0,100],
            hue_order=hue_order, 
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'])
            
sns.stripplot(data=all_data, x="Segment",y="percent",hue='Group', hue_order=hue_order, dodge=True,
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],size=6, color=".3")            
            
sns.move_legend(ax,"upper right")
plt.rcParams['pdf.use14corefonts'] = True
sns.despine()
plt.savefig('boxplot_clusterpercent',format='pdf', bbox_inches="tight")
plt.show()
```
```{python}
#plot but separate by genotype and manipulation 
sns.set()
sns.set_theme("talk")
sns.set_theme(style="ticks")

fig,axes=plt.subplots(2,2,figsize=(10,12),sharey=True)
fig.subplots_adjust(hspace=0.7,wspace=0.125)
#WT 
WT=all_data.loc[all_data['Group']=='WT']
WT_MD=all_data.loc[all_data['Group']=='WT_MD']
KO=all_data.loc[all_data['Group']=='KO']
KO_MD=all_data.loc[all_data['Group']=='KO_MD']

hue_palette=sns.color_palette("rocket", 4)

sns.boxplot(data=WT, x="Segment",y="percent",ax=axes[0,0],
            whis=[0,100],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],hue='Segment',palette=hue_palette)


sns.boxplot(data=KO, x="Segment",y="percent",ax=axes[0,1],
            whis=[0,100],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],hue='Segment',palette=hue_palette)

sns.boxplot(data=WT_MD, x="Segment",y="percent",ax=axes[1,0],
            whis=[0,100],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],hue='Segment',palette=hue_palette)

sns.boxplot(data=KO_MD, x="Segment",y="percent",ax=axes[1,1],
            whis=[0,100],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],hue='Segment',palette=hue_palette)

            
sns.stripplot(data=WT, x="Segment",y="percent",ax=axes[0,0],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],size=4, color=".3")
sns.stripplot(data=KO, x="Segment",y="percent",ax=axes[0,1],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],size=4, color=".3")
sns.stripplot(data=WT_MD, x="Segment",y="percent",ax=axes[1,0],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],size=4, color=".3")
sns.stripplot(data=KO_MD, x="Segment",y="percent",ax=axes[1,1],
            order=['amoeboid', 'rod-like','ramified','hyper-ramified'],size=4, color=".3")
sns.despine() 
axes[0,0].set_title('WT no MD')
axes[0,1].set_title('cKO no MD')
axes[1,0].set_title('WT 5D MD')
axes[1,1].set_title('cKO 5D MD')
plt.rcParams['pdf.use14corefonts'] = True

plt.savefig('boxplot_clustersbygroup',format='pdf', bbox_inches="tight")

plt.show()

```



Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
